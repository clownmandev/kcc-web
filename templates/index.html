import os
import subprocess
import shutil
import requests
import sys
import time
import re
import zipfile
import glob
import uuid
from urllib.parse import urlparse
from flask import Flask, render_template, request, send_file, jsonify, Response, stream_with_context
from werkzeug.utils import secure_filename

app = Flask(__name__)

# --- CONFIGURATION ---
BASE_DIR = '/tmp'
UPLOAD_FOLDER = os.path.join(BASE_DIR, 'kcc_uploads')
OUTPUT_FOLDER = os.path.join(BASE_DIR, 'kcc_output')
DOWNLOAD_FOLDER = os.path.join(BASE_DIR, 'kcc_downloads')
ZIP_TEMP = os.path.join(BASE_DIR, 'kcc_temp_zips')
COMBINE_DIR = os.path.join(BASE_DIR, 'kcc_combined')

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# Ensure directories exist
for d in [UPLOAD_FOLDER, OUTPUT_FOLDER, DOWNLOAD_FOLDER, ZIP_TEMP, COMBINE_DIR]:
    os.makedirs(d, exist_ok=True)

# --- HELPER FUNCTIONS ---

def safe_clean_folder(path):
    """Try to delete a folder, but don't crash if it fails."""
    try:
        if os.path.exists(path):
            shutil.rmtree(path)
    except Exception as e:
        print(f"WARNING: Could not clean up {path}: {e}")

def run_command_with_retry(cmd, max_retries=3):
    """Runs a shell command and yields output. Retries on failure."""
    attempt = 0
    while attempt < max_retries:
        try:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)
            for line in process.stdout:
                yield line
            process.wait()
            if process.returncode == 0:
                return
            yield f"WARNING: Process failed with code {process.returncode}. Retrying ({attempt+1}/{max_retries})..."
        except Exception as e:
            yield f"ERROR: System execution failed: {str(e)}"
        attempt += 1
        time.sleep(2)
    yield "FAILURE: Max retries reached."

def is_image_dir(path):
    if not os.path.isdir(path): return False
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.gif']:
        if glob.glob(os.path.join(path, ext)):
            return True
    return False

def scrape_website_images(url, save_folder):
    domain = urlparse(url).netloc
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Referer': f"https://{domain}/" 
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        html = response.text
        img_urls = re.findall(r'(?:src|data-src)="([^"]+?\.(?:jpg|jpeg|png|webp))"', html)
        img_urls = list(dict.fromkeys(img_urls))
        
        if not img_urls:
            yield "LOG: No images found. Ensure this is a CHAPTER URL.\n"
            return

        yield f"LOG: Found {len(img_urls)} images. Downloading...\n"

        for i, img_url in enumerate(img_urls):
            try:
                if not img_url.startswith('http'): continue 
                img_name = f"page_{i:04d}.jpg"
                img_save_path = os.path.join(save_folder, img_name)
                
                with requests.get(img_url, headers=headers, stream=True, timeout=10) as r:
                    r.raise_for_status()
                    with open(img_save_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=1024*1024):
                            f.write(chunk)
                
                if i % 5 == 0: yield f"LOG: Downloaded page {i+1}/{len(img_urls)}\n"
            except Exception as e:
                yield f"LOG: Failed to download image {i+1}: {e}\n"

    except Exception as e:
        yield f"ERROR: Scraping failed: {e}\n"

# --- API ROUTES ---

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/search', methods=['POST'])
def search_manga():
    query = request.form.get('query')
    if not query: return jsonify({'error': 'No query provided'}), 400
    
    url = "https://api.mangadex.org/manga"
    params = {
        'title': query, 
        'limit': 10, 
        'contentRating[]': ['safe', 'suggestive', 'erotica', 'pornographic'], 
        'order[relevance]': 'desc',
        'includes[]': ['cover_art']
    }
    try:
        r = requests.get(url, params=params)
        data = r.json()
        results = []
        for manga in data.get('data', []):
            attr = manga['attributes']
            title = attr['title'].get('en') or list(attr['title'].values())[0]
            desc = attr['description'].get('en', 'No description available.')
            cover_file = None
            for rel in manga.get('relationships', []):
                if rel['type'] == 'cover_art':
                    cover_file = rel['attributes']['fileName']
                    break
            cover_url = f"https://uploads.mangadex.org/covers/{manga['id']}/{cover_file}.256.jpg" if cover_file else "
